# @package _global_
defaults:
  - override /pipeline: nn
  - override /pipeline/lr_scheduler: ExponentialLR
  - override /hydra/launcher: submitit_slurm
  - override /hydra/sweeper: optuna
  - override /hydra/sweeper/sampler: tpe
  - override /split: cv
  - override /tracking: mlflow_dagshub
  
early_stopping: null
job_name: hopt_dnn

hydra:
  job:
    name: ${job_name}
  sweep:
    dir: ${dir.out}/${job_name}
    subdir: ${run_uid}
  launcher:
    gres: "gpu:1"
    cpus_per_task: 4
    mem_gb: 10
    timeout_min: 10
  sweeper:
    n_jobs: 8
    n_trials: 64
    study_name: ${job_name}-001
    params:
      pipeline.optimizer.lr: tag(log, interval(0.001, 0.01))
      pipeline.optimizer.weight_decay: tag(log, interval(0.0001, 0.001))
      pipeline.model.dropout: interval(0.1, 0.5)
      pipeline.lr_scheduler.gamma: choice(0.98,0.99,0.999)

seed: null
do_log_model: False

data_processor_artifact:
  log: False

split:
  n_folds: 4

trainer:
  max_epochs: 100

model:
  n_hidden: 2
  hidden_size: 4096

tracking:
  experiment_name: hopt_dnn